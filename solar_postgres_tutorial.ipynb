{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79b155f",
   "metadata": {},
   "source": [
    "# Tutorial: Esquema PostgreSQL para proyectos solares (desde Python)\n",
    "\n",
    "**Objetivo:**  \n",
    "Crear un esquema en PostgreSQL desde Python que modele proyectos solares, sistemas por proyecto, y datos meteorológicos y eléctricos por sistema. Mostrar cómo cargar CSV eléctricos con `pandas` y `psycopg2` usando inserciones masivas.\n",
    "\n",
    "**Nota para la clase:** cada celda contiene explicación y código ejecutable. Asegúrate de ajustar las credenciales de la base de datos antes de ejecutar las celdas que crean el esquema o insertan datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ac093",
   "metadata": {},
   "source": [
    "## Requisitos\n",
    "\n",
    "- Python 3.8+  \n",
    "- Paquetes: `psycopg2-binary`, `pandas`\n",
    "\n",
    "Instálalos ejecutando la celda siguiente (usa `%pip` en notebooks para asegurar que se instalen en el kernel actual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f875073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si no tienes psycopg2-binary y pandas, descomenta la línea anterior y ejecútala.\n"
     ]
    }
   ],
   "source": [
    "# Instala las dependencias (descomenta si necesitas instalar)\n",
    "# %pip install psycopg2-binary pandas\n",
    "print('Si no tienes psycopg2-binary y pandas, descomenta la línea anterior y ejecútala.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79b5d1",
   "metadata": {},
   "source": [
    "## Importaciones y configuración básica\n",
    "\n",
    "Define imports y la configuración de conexión (modifica `DB_CONFIG` con tus credenciales).\n",
    "**No** compartas contraseñas reales en notebooks públicos; en clase puedes pedir a cada estudiante que use sus credenciales locales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b86d7339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports y configuración listos. Revisa DB_CONFIG antes de crear el esquema.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------------\n",
    "# CONFIGURA TUS CREDENCIALES AQUÍ\n",
    "# -------------------------\n",
    "# Cambia estos valores por los de tu servidor PostgreSQL antes de ejecutar la creación del esquema.\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"solar_db\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"clase2021\"\n",
    "}\n",
    "\n",
    "def _conn_str(cfg: Dict[str, Any]) -> str:\n",
    "    return f\"host={cfg['host']} port={cfg['port']} dbname={cfg['dbname']} user={cfg['user']} password={cfg['password']}\"\n",
    "\n",
    "def get_connection(cfg: Optional[Dict[str, Any]] = None):\n",
    "    import psycopg2\n",
    "    cfg = cfg or DB_CONFIG\n",
    "    conn = psycopg2.connect(_conn_str(cfg))\n",
    "    return conn\n",
    "\n",
    "print('Imports y configuración listos. Revisa DB_CONFIG antes de crear el esquema.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa8892",
   "metadata": {},
   "source": [
    "## Crear esquema y tablas en PostgreSQL\n",
    "\n",
    "Esta celda crea tablas: `projects`, `systems`, `meteo_data` y `electrical_data`, y los índices necesarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "848ca552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función create_schema definida. Ejecuta create_schema(DB_CONFIG) para crear tablas.\n"
     ]
    }
   ],
   "source": [
    "def create_schema(cfg: Optional[Dict[str, Any]] = None, schema_name: str = 'public'):\n",
    "    conn = get_connection(cfg)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_name}.projects (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        location TEXT,\n",
    "        description TEXT,\n",
    "        created_at TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS {schema_name}.systems (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        project_id INTEGER NOT NULL REFERENCES {schema_name}.projects(id) ON DELETE CASCADE,\n",
    "        name TEXT NOT NULL,\n",
    "        capacity_kw DOUBLE PRECISION,\n",
    "        inverter_type TEXT,\n",
    "        notes TEXT,\n",
    "        created_at TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS {schema_name}.meteo_data (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        system_id INTEGER NOT NULL REFERENCES {schema_name}.systems(id) ON DELETE CASCADE,\n",
    "        timestamp TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "        ghi DOUBLE PRECISION,\n",
    "        dni DOUBLE PRECISION,\n",
    "        dhi DOUBLE PRECISION,\n",
    "        temp_c DOUBLE PRECISION,\n",
    "        wind_m_s DOUBLE PRECISION,\n",
    "        precip_mm DOUBLE PRECISION,\n",
    "        source TEXT,\n",
    "        inserted_at TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS {schema_name}.electrical_data (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        system_id INTEGER NOT NULL REFERENCES {schema_name}.systems(id) ON DELETE CASCADE,\n",
    "        timestamp TIMESTAMP WITH TIME ZONE NOT NULL,\n",
    "        power_kw DOUBLE PRECISION,\n",
    "        voltage_v DOUBLE PRECISION,\n",
    "        current_a DOUBLE PRECISION,\n",
    "        energy_kwh DOUBLE PRECISION,\n",
    "        status TEXT,\n",
    "        inserted_at TIMESTAMP WITH TIME ZONE DEFAULT now()\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(f\"CREATE INDEX IF NOT EXISTS idx_systems_project ON {schema_name}.systems(project_id);\")\n",
    "    cur.execute(f\"CREATE INDEX IF NOT EXISTS idx_meteo_system_time ON {schema_name}.meteo_data(system_id, timestamp);\")\n",
    "    cur.execute(f\"CREATE INDEX IF NOT EXISTS idx_elec_system_time ON {schema_name}.electrical_data(system_id, timestamp);\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print('Esquema y tablas creadas en PostgreSQL.')\n",
    "\n",
    "print('Función create_schema definida. Ejecuta create_schema(DB_CONFIG) para crear tablas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1153e",
   "metadata": {},
   "source": [
    "## Funciones CRUD básicas\n",
    "\n",
    "Funciones para insertar proyectos y sistemas en la base de datos (devuelven el id insertado).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "503d563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones add_project y add_system definidas.\n"
     ]
    }
   ],
   "source": [
    "def add_project(name: str, location: Optional[str] = None, description: Optional[str] = None, cfg: Optional[Dict[str, Any]] = None) -> int:\n",
    "    conn = get_connection(cfg)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO projects (name, location, description) VALUES (%s, %s, %s) RETURNING id;\", (name, location, description))\n",
    "    project_id = cur.fetchone()[0]\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return project_id\n",
    "\n",
    "def add_system(project_id: int, name: str, capacity_kw: Optional[float] = None,\n",
    "               inverter_type: Optional[str] = None, notes: Optional[str] = None, cfg: Optional[Dict[str, Any]] = None) -> int:\n",
    "    conn = get_connection(cfg)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        \"INSERT INTO systems (project_id, name, capacity_kw, inverter_type, notes) VALUES (%s, %s, %s, %s, %s) RETURNING id;\",\n",
    "        (project_id, name, capacity_kw, inverter_type, notes)\n",
    "    )\n",
    "    system_id = cur.fetchone()[0]\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return system_id\n",
    "\n",
    "print('Funciones add_project y add_system definidas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a7fbc",
   "metadata": {},
   "source": [
    "## Helpers y sanitización\n",
    "\n",
    "Funciones internas para convertir valores leídos del CSV a tipos seguros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1269101e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers definidos.\n"
     ]
    }
   ],
   "source": [
    "def _safe_float(x) -> Optional[float]:\n",
    "    try:\n",
    "        if x is None:\n",
    "            return None\n",
    "        s = str(x).strip()\n",
    "        if s == '' or s.lower() in ('nan', 'none', 'null'):\n",
    "            return None\n",
    "        s = s.replace(',', '.')\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _safe_str(x) -> Optional[str]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    return s if s != '' else None\n",
    "\n",
    "print('Helpers definidos.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8086d2e",
   "metadata": {},
   "source": [
    "## Ingestión de CSV a `electrical_data` (bulk insert)\n",
    "\n",
    "La función siguiente lee un CSV con `pandas` por chunks y realiza inserciones masivas usando `psycopg2.extras.execute_values`.\n",
    "Ajusta `col_mappings` si los nombres de columnas del CSV son distintos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f40441f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_electrical_csv(cfg: Optional[Dict[str, Any]], system_id: int, csv_path: str,\n",
    "                          col_mappings: Optional[Dict[str, str]] = None,\n",
    "                          chunk_size: int = 10000):\n",
    "    import psycopg2\n",
    "    import psycopg2.extras\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"No existe el archivo CSV: {csv_path}\")\n",
    "\n",
    "    default_map = {\n",
    "        'timestamp': 'timestamp',\n",
    "        'time': 'timestamp',\n",
    "        'datetime': 'timestamp',\n",
    "        'power_kw': 'power_kw',\n",
    "        'power_kW': 'power_kw',\n",
    "        'power': 'power_kw',\n",
    "        'voltage_v': 'voltage_v',\n",
    "        'voltage': 'voltage_v',\n",
    "        'current_a': 'current_a',\n",
    "        'current': 'current_a',\n",
    "        'energy_kwh': 'energy_kwh',\n",
    "        'energy': 'energy_kwh',\n",
    "        'status': 'status'\n",
    "    }\n",
    "    combined_map = {**default_map, **(col_mappings or {})}\n",
    "\n",
    "    total_inserted = 0\n",
    "    for chunk in pd.read_csv(csv_path, chunksize=chunk_size):\n",
    "        chunk.columns = [c.strip() for c in chunk.columns]\n",
    "        map_existing = {}\n",
    "        for c in chunk.columns:\n",
    "            lower = c.lower()\n",
    "            if lower in combined_map:\n",
    "                map_existing[c] = combined_map[lower]\n",
    "            elif lower in {'timestamp', 'power_kw', 'voltage_v', 'current_a', 'energy_kwh', 'status'}:\n",
    "                map_existing[c] = lower\n",
    "\n",
    "        if not any(v == 'timestamp' for v in map_existing.values()):\n",
    "            raise ValueError(f\"El CSV no contiene columna de timestamp reconocida. Columnas: {list(chunk.columns)}\")\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for src, target in map_existing.items():\n",
    "            df[target] = chunk[src]\n",
    "\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True, errors='coerce')\n",
    "        n_before = len(df)\n",
    "        df = df.dropna(subset=['timestamp'])\n",
    "        n_after = len(df)\n",
    "        if n_after < n_before:\n",
    "            print(f\"Descartadas {n_before-n_after} filas con timestamp inválido en chunk.\")\n",
    "\n",
    "        for col in ['power_kw', 'voltage_v', 'current_a', 'energy_kwh', 'status']:\n",
    "            if col not in df.columns:\n",
    "                df[col] = None\n",
    "\n",
    "        tuples = [\n",
    "            (\n",
    "                system_id,\n",
    "                pd.Timestamp(row['timestamp']).to_pydatetime(),\n",
    "                _safe_float(row['power_kw']),\n",
    "                _safe_float(row['voltage_v']),\n",
    "                _safe_float(row['current_a']),\n",
    "                _safe_float(row['energy_kwh']),\n",
    "                _safe_str(row['status'])\n",
    "            )\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "        conn = get_connection(cfg)\n",
    "        cur = conn.cursor()\n",
    "        insert_sql = \"\"\"\n",
    "            INSERT INTO electrical_data (system_id, timestamp, power_kw, voltage_v, current_a, energy_kwh, status)\n",
    "            VALUES %s\n",
    "        \"\"\"\n",
    "        try:\n",
    "            psycopg2.extras.execute_values(cur, insert_sql, tuples, template=None, page_size=1000)\n",
    "            conn.commit()\n",
    "            inserted = len(tuples)\n",
    "            total_inserted += inserted\n",
    "            print(f\"Insertadas {inserted} filas (acumulado: {total_inserted})\")\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            raise RuntimeError(f\"Error insertando chunk: {e}\")\n",
    "        finally:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "\n",
    "    print(f\"Ingestión finalizada. Total filas insertadas: {total_inserted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25100e",
   "metadata": {},
   "source": [
    "## Generar un CSV de ejemplo (local) y probar la ingestión\n",
    "\n",
    "La siguiente celda genera un CSV sintético en la carpeta del notebook y lo ingresa al `system_id` creado en el ejemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3171b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV de ejemplo creado: datos_electricos_ejemplo_pg.csv\n"
     ]
    }
   ],
   "source": [
    "# Generar CSV de ejemplo\n",
    "import pandas as pd\n",
    "sample_csv = 'datos_electricos_ejemplo_pg.csv'\n",
    "from datetime import timedelta\n",
    "start = datetime(2025, 10, 1, 0, 0)\n",
    "rows = []\n",
    "for i in range(24):\n",
    "    t = start + timedelta(hours=i)\n",
    "    power = max(0, 500 * (1 - abs(12 - i)/12))\n",
    "    voltage = 400 + (i % 3)\n",
    "    current = power / (voltage if voltage != 0 else 1)\n",
    "    energy = power / 1000.0\n",
    "    status = 'OK' if power > 0 else 'NO_GEN'\n",
    "    rows.append([t.strftime('%Y-%m-%d %H:%M:%S'), round(power,2), round(voltage,2), round(current,3), round(energy,4), status])\n",
    "\n",
    "pd.DataFrame(rows, columns=['timestamp','power_kW','voltage','current','energy','status']).to_csv(sample_csv, index=False)\n",
    "print('CSV de ejemplo creado:', sample_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc6ab5",
   "metadata": {},
   "source": [
    "## Ejemplo completo: crear esquema, insertar proyecto/sistema e ingestar CSV\n",
    "\n",
    "Ejecuta esta celda **después** de haber ajustado `DB_CONFIG` y de haber creado el esquema (o puedes dejar que la celda cree el esquema).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "363aed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esquema y tablas creadas en PostgreSQL.\n",
      "Proyecto creado id: 1\n",
      "Sistema creado id: 1\n",
      "Insertadas 24 filas (acumulado: 24)\n",
      "Ingestión finalizada. Total filas insertadas: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carloscm\\AppData\\Local\\Temp\\ipykernel_20688\\4115904817.py:44: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# EJEMPLO: crear esquema y agregar datos de ejemplo\n",
    "# Asegúrate de haber ajustado DB_CONFIG en la celda de configuración.\n",
    "\n",
    "# try:\n",
    "create_schema(DB_CONFIG)\n",
    "p_id = add_project('Parque Solar Postgres Demo', location='Campus', description='Demo PostgreSQL')\n",
    "print('Proyecto creado id:', p_id)\n",
    "s_id = add_system(p_id, 'Array Demo', capacity_kw=1200.0, inverter_type='String')\n",
    "print('Sistema creado id:', s_id)\n",
    "\n",
    "# Ingestar CSV generado en la celda anterior\n",
    "ingest_electrical_csv(DB_CONFIG, s_id, sample_csv)\n",
    "# except Exception as e:\n",
    "#     print('Error en el flujo de ejemplo:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bbdb08",
   "metadata": {},
   "source": [
    "## Consultas simples\n",
    "\n",
    "Ejemplo de cómo leer algunas filas desde `electrical_data` para verificar que la ingestión funcionó.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultar las primeras filas del sistema creado (si existe)\n",
    "try:\n",
    "    conn = get_connection(DB_CONFIG)\n",
    "    cur = conn.cursor()\n",
    "    s_id = globals().get('s_id', None)\n",
    "    if s_id is None:\n",
    "        print('s_id no encontrado en el entorno. Ejecuta la celda de ejemplo que crea el sistema.')\n",
    "    else:\n",
    "        cur.execute('SELECT id, system_id, timestamp, power_kw, voltage_v, current_a, energy_kwh, status FROM electrical_data WHERE system_id = %s ORDER BY timestamp LIMIT 10;', (s_id,))\n",
    "        rows = cur.fetchall()\n",
    "        for r in rows:\n",
    "            print(r)\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print('No se pudo ejecutar la consulta:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33784bc4",
   "metadata": {},
   "source": [
    "---\n",
    "### Sugerencias para la clase\n",
    "\n",
    "- Pide a los estudiantes modificar `col_mappings` si cambian nombres de columnas en el CSV.  \n",
    "- Añade validaciones (rangos físicos, duplicados por timestamp).  \n",
    "- Para producción: usar pool de conexiones, manejo de transacciones más fino, y monitoreo.  \n",
    "- Si quieres, puedo convertir este notebook en una versión con celdas que muestren gráficos (matplotlib) de potencia a lo largo del día.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
